class AutoExtractor:
    def __init__(self):
        import re, pickle, os
        self.reID = re.compile(''' id=("[^"']+"|'[^']+')''', re.I)
        self.reClass = re.compile(''' class=("[^"']+"|'[^']+')''', re.I)
        self.reIdClass = re.compile(''' (id|class)=("[^"']+"|'[^']+')''', re.I)
        self.reNewline = re.compile('[\r\n]{2,}')
        self.templateCachePath = 'elementTemplate/'
        self.commonThreshold = 0.4
        self.maxLookup = 10
        try: os.mkdir(self.templateCachePath)
        except: pass
        self.elementTemplateCache = {}
 

    def fetchHTML(self, url):
        '''웹 페이지를 가져오는 함수'''
        import urllib, urllib.request, urllib.parse, random
        try:
            t = urllib.parse.urlparse(url)
            params = [(k, v[0]) for k, v in urllib.parse.parse_qs(t.query).items()]
            url = t.scheme + '://' + t.netloc + t.path + '?' + urllib.parse.urlencode(params)
            headerList = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36',
                          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36',
                          'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko']
            req = urllib.request.Request(
                url,
                headers={
                    'User-Agent': random.sample(headerList, 1)[0]
                }
            )
            f = urllib.request.urlopen(req)
            newUrl = f.geturl()
            rawbytes = f.read()
            f.close()
            try:
                return self.refineDuplicatedHTMLId(rawbytes.decode('utf-8')), newUrl
            except UnicodeDecodeError:
                return self.refineDuplicatedHTMLId(rawbytes.decode('euc-kr')), newUrl
        except:
            raise RuntimeError('Cannot open ' + url)

    def refineDuplicatedHTMLId(self, html):
        '''일부 비표준 사이트에서는 엘리먼트 ID를 중복해서 사용합니다. 
        이 경우 각 ID를 유니크하게 바꿔주는 함수'''
        import re
        dups = {}
        from collections import Counter
        for k, v in Counter(id[1:-1] for id in self.reID.findall(html)).most_common():
            if v <= 1: break
            dups[k] = v
        for id, n in dups.items():
            for i in range(n):
                html = re.sub(''' id=('%s'|"%s")''' % (id, id), ' id="%s___%d"' % (id, i), html, 1)
        return html
    
    def waitOpenFile(self, path):
        import os.path, time
        if not os.path.exists(path) or not os.path.isfile(path): return
        for _ in range(10):
            try:
                f = open(path, 'rb')
                return f
            except IOError:
                time.sleep(10)
        raise IOError("Failed to open '%s'" % path)   

        
    def getElementTemplate(self, url, maxLookup = 0):
        '''본문 템플릿을 캐싱하고 가져오는 함수'''
        import time, pickle, os, re
        if maxLookup == 0: maxLookup = self.maxLookup
        # first, search in memory
        for k, v in self.elementTemplateCache.items():
            if url.startswith(k): return v
 
        # second, search in local file
        try:
            nurl = re.sub('[:/]', '_', url)
            for name in os.listdir(self.templateCachePath):
                if nurl.startswith(name):
                    with self.waitOpenFile(self.templateCachePath + name) as f:
                        base, elements = pickle.load(f)
                        self.elementTemplateCache[base] = elements
                        return elements
        except: pass
 
        # else, build and save template
        

    def extractText(self, url):
        '''위의 함수들을 종합하여, 실제 본문을 추출해주는 함수'''
        import bs4
        html, url = self.fetchHTML(url)
        soup = bs4.BeautifulSoup(html, "lxml")
        tmplt = self.getElementTemplate(url)
        [s.extract() for s in soup('script')]
        [s.extract() for s in soup('style')]
        [s.extract() for s in soup('iframe')]
        [s.extract() for s in soup('a')]
        [s.extract() for s in soup('li')]
        [s.extract() for s in soup('header')]
        [s.extract() for s in soup('footer')]
        text1 = re.sub("&nbsp; | &nbsp;|\n|\t|\r",'',soup.text)
        text2 = re.sub('\n\n','',text1)
       
        
        return self.reNewline.sub('\n', text2.strip()), url


if __name__ == "__main__":
    import sys, re, time, traceback
    import site_list
    
    ae = AutoExtractor()
    txt, url = ae.extractText('https://www.kbstar.com/')
    site_text = re.sub('\s+', ' ', txt)
    list_site_text = site_text.split()
    choice_box = []
    for i in list_site_text:
        for key, value in site_list.SITE_LIST['BANK'].items():
            if i in value:
                choice_box.append(key)
                
    
    max_num = max(set(choice_box),key=choice_box.count)
    print(max_num,'사칭앱')
    
    #print(url, re.sub('\s+', ' ', txt), sep='\t')
    